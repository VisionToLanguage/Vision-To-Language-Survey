<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Vision to Language: A Literature Survey by VisionToLanguage</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Vision to Language: A Literature Survey</h1>
      <h2 class="project-tagline">vision language caption NLP</h2>
      <a href="https://github.com/VisionToLanguage/Vision-To-Language-Survey" class="btn">View on GitHub</a>
      <a href="https://github.com/VisionToLanguage/Vision-To-Language-Survey/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/VisionToLanguage/Vision-To-Language-Survey/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h2>
<a id="1-introduction" class="anchor" href="#1-introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>1. Introduction</strong>
</h2>

<h2>
<a id="2-video-captioning" class="anchor" href="#2-video-captioning" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>2. Video Captioning</strong>
</h2>

<ul>
<li><p><strong>Training a Multilingual Sportscaster: Using Perceptual Context to Learn Language</strong>.
David L. Chen, Joohyun Kim, Raymond J. Mooney.
In Journal of Artificial Intelligence Research (JAIR) , 37, pages 397-435, 2010.
<a href="http://www.cs.utexas.edu/%7Eml/clamp/sportscasting/">[<strong>Project Page</strong>]</a>
<a href="http://dl.acm.org/citation.cfm?id=1861761">[ACM]</a>
<a href="https://www.jair.org/media/2962/live-2962-4903-jair.pdf">[PDF]</a>
<a href="http://www.jair.org/papers/paper2962.html">[JAIR link]</a></p></li>
<li><p><strong>Grounded Language Learning from Video Described with Sentences</strong>.
H. Yu and J. M. Siskind.
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, 2013, <em>best paper award</em>.
<a href="http://haonanyu.com/research/acl2013/">[<strong>Project Page</strong>]</a>
<a href="http://haonanyu.com/research/acl2013/">[PDF]</a></p></li>
<li><p><strong>Story-Driven Summarization for Egocentric Video</strong>.
Zheng Lu and Kristen Grauman.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Portland, OR, June 2013.
<a href="http://vision.cs.utexas.edu/projects/egocentric/storydriven.html">[<strong>Project Page</strong>]</a>
<a href="http://www.cs.utexas.edu/%7Egrauman/papers/lu-grauman-cvpr2013.pdf">[PDF]</a></p></li>
<li><p><strong>Movie Script Summarization as Graph-based Scene Extraction</strong>.
Philip John Gorinski and Mirella Lapata.
Proc. Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL (NAACL 2015), pages 1066–1076.
May 31 – June 5, 2015.
<a href="http://www.aclweb.org/anthology/N/N15/N15-1113.pdf">[PDF]</a></p></li>
<li><p><strong>Collecting Highly Parallel Data for Paraphrase Evaluation</strong>
David L. Chen and William B. Dolan.
Annual Meetings of the Association for Computational Linguistics (ACL), 2011.
<a href="http://www.cs.utexas.edu/users/ml/clamp/videoDescription/">[<strong>Project Page</strong>]</a>
<a href="http://www.cs.utexas.edu/users/ml/papers/chen.acl11.pdf">[PDF]</a></p></li>
<li><p><strong>What’s Cookin’? Interpreting Cooking Videos using Text, Speech and Vision</strong>.
Jonathan Malmaud, Jonathan Huang, Vivek Rathod, Nick Johnston, Andrew Rabinovich, and Kevin Murphy.
NAACL 2015.
<a href="http://www.cs.ubc.ca/%7Emurphyk/Papers/naacl15.pdf">[PDF]</a></p></li>
<li><p><strong>Discriminative Unsupervised Alignment of Natural Language Instructions with Corresponding Video Segments</strong>.
I. Naim, Y. Song, Q. Liu, L. Huang, H. Kautz, J. Luo, and D. Gildea. 
Proc. NAACL 2015.
<a href="http://acl.cs.qc.edu/%7Elhuang/papers/naim-video.pdf">[PDF]</a></p></li>
<li><p><strong>Grounding Action Descriptions in Videos</strong>.
Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal.
TACL 2013.
<a href="http://www.aclweb.org/anthology/Q13-1003">[PDF]</a></p></li>
<li><p><strong>Translating Videos to Natural Language Using Deep Recurrent Neural Networks</strong>.
Subhashini Venugopalan, Huijun Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko.
North American Chapter of the Association for Computational Linguistics, Denver, Colorado, June 2015. (NAACL-HLT 2015)
<a href="https://www.cs.utexas.edu/%7Evsub/pdf/Translating_Videos_NAACL15.pdf">[PDF]</a>
<a href="https://github.com/vsubhashini/caffe/tree/recurrent/examples/youtube">[Code]</a></p></li>
</ul>

<h2>
<a id="3-image-captioning" class="anchor" href="#3-image-captioning" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>3. Image Captioning</strong>
</h2>

<ul>
<li><p>SBU Captioned Photo Dataset: <strong>Im2Text: Describing Images Using 1 Million Captioned Photograph.</strong>
Vicente Ordonez, Girish Kulkarni, Tamara L. Berg. s.
Neural Information Processing Systems(NIPS), 2011.
<a href="http://tlberg.cs.unc.edu/vicente/sbucaptions/">[<strong>Project Page</strong>]</a>
<a href="http://tamaraberg.com/papers/generation_nips2011.pdf">[PDF]</a></p></li>
<li><p>Flickr's 100 million images dataset 2012 (YFCC100M)
<a href="http://labs.yahoo.com/news/yfcc100m/">[<strong>Project Page</strong>]</a></p></li>
</ul>

<h3>
<a id="captioned-by-crowd" class="anchor" href="#captioned-by-crowd" aria-hidden="true"><span class="octicon octicon-link"></span></a>Captioned by Crowd</h3>

<ul>
<li>
<p>PASCAL sentences dataset (1K)
(7) Flickr 8K 
(8) Flickr 30K
(8a) Flickr 30K
--&gt; \cite{Plummer2015flickrentities}
(9) Michel has worked on with Mark Yatskar (*SEM?) 2014
--&gt; \cite{yatskar2014dense} 
(10) COCO dataset</p>

<p>(11) Elliot &amp; Keller Visual Dependency Graph (VDG) work 
--&gt; \cite{elliott2013visualdependency}</p>
</li>
</ul>

<h3>
<a id="already-captioned" class="anchor" href="#already-captioned" aria-hidden="true"><span class="octicon octicon-link"></span></a>Already Captioned</h3>

<pre><code>(12) Yejin's new dataset (expressive language) - NAACL 2015; Deja Image Captions
--&gt; \cite{chen2015deja} 
</code></pre>

<h2>
<a id="4-beyond-image-captioning" class="anchor" href="#4-beyond-image-captioning" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>4. Beyond Image Captioning</strong>
</h2>

<p>(13) Bergs' MadLibs (and QA?) <a href="http://arxiv.org/pdf/1506.00278.pdf">http://arxiv.org/pdf/1506.00278.pdf</a> \cite{yu2015vml}
(14) Bergs' ReferIt dataset (EMNLP 2014) <a href="http://tamaraberg.com/referitgame/">http://tamaraberg.com/referitgame/</a> \cite{kazemzadeh2014referitgame}
    (15) VQA (V. tech) - ICCV 2015 \cite{antol2015vqa}
        -- 10K sampled (somehow) from MS COCO
    (16) mQA (might not be released yet?) Baidu - 2015 - captions converted to QA <a href="http://arxiv.org/pdf/1505.05612.pdf">http://arxiv.org/pdf/1505.05612.pdf</a>, NIPS 2015
    (17) The “Image Question Answering” dataset from Toronto: \cite{ren2015imageqa}
    (18) dataset from ("Joint Photo Stream and Blog Post Summarization and Exploration" and "Ranking and Retrieval of Image Sequences from Multiple Paragraph Queries"), CVPR 2015
    (19) Disney dataset (check that is same as above/hasn't changed)
    (20) Devi Parikh &amp; students: fill-in-the-blanks and visual paraphrasing (page under constructions, datasets and code will be online here: <a href="https://filebox.ece.vt.edu/%7Elinxiao/imagine/">https://filebox.ece.vt.edu/~linxiao/imagine/</a>), CVPR 2015 \cite{lin2015imagine}
%%Note: move (21) to videos
    (21) Book2Movie (CVPR, 2015): \cite{tapaswi2015book2movie}</p>

<h2>
<a id="5-more-possibilities" class="anchor" href="#5-more-possibilities" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>5. More Possibilities</strong>
</h2>

<pre><code>(22) What, Where, Who? (ICCV 2007) \cite{fei2010whathwherewho}
(23) Ramanath event-centric paper (with Fei Fei) (ICCV 2013)
(24) Ontology of events and social settings (Karpathy and Fei Fei 2015)
</code></pre>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/VisionToLanguage/Vision-To-Language-Survey">Vision to Language: A Literature Survey</a> is maintained by <a href="https://github.com/VisionToLanguage">VisionToLanguage</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>

