<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Vision to Language: A Literature Survey by VisionToLanguage</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Vision to Language: A Literature Survey</h1>
      <h2 class="project-tagline">vision language caption NLP</h2>
      <a href="https://github.com/VisionToLanguage/Vision-To-Language-Survey" class="btn">View on GitHub</a>
      <a href="https://github.com/VisionToLanguage/Vision-To-Language-Survey/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/VisionToLanguage/Vision-To-Language-Survey/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h2>
<a id="1-introduction" class="anchor" href="#1-introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>1. Introduction</strong>
</h2>

<h2>
<a id="2-video-captioning" class="anchor" href="#2-video-captioning" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>2. Video Captioning</strong>
</h2>

<ul>
<li><p><strong>Training a Multilingual Sportscaster: Using Perceptual Context to Learn Language</strong>.
David L. Chen, Joohyun Kim, Raymond J. Mooney.
In Journal of Artificial Intelligence Research (JAIR) , 37, pages 397-435, 2010.
<a href="http://www.cs.utexas.edu/%7Eml/clamp/sportscasting/">[<strong>Project Page</strong>]</a>
<a href="http://dl.acm.org/citation.cfm?id=1861761">[ACM]</a>
<a href="https://www.jair.org/media/2962/live-2962-4903-jair.pdf">[PDF]</a>
<a href="http://www.jair.org/papers/paper2962.html">[JAIR link]</a></p></li>
<li><p><strong>Grounded Language Learning from Video Described with Sentences</strong>.
H. Yu and J. M. Siskind.
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, 2013, <em>best paper award</em>.
<a href="http://haonanyu.com/research/acl2013/">[<strong>Project Page</strong>]</a>
<a href="http://haonanyu.com/research/acl2013/">[PDF]</a></p></li>
<li><p><strong>Story-Driven Summarization for Egocentric Video</strong>.
Zheng Lu and Kristen Grauman.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Portland, OR, June 2013.
<a href="http://vision.cs.utexas.edu/projects/egocentric/storydriven.html">[<strong>Project Page</strong>]</a>
<a href="http://www.cs.utexas.edu/%7Egrauman/papers/lu-grauman-cvpr2013.pdf">[PDF]</a></p></li>
<li><p><strong>Movie Script Summarization as Graph-based Scene Extraction</strong>.
Philip John Gorinski and Mirella Lapata.
Proc. Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL (NAACL 2015), pages 1066–1076.
May 31 – June 5, 2015.
<a href="http://www.aclweb.org/anthology/N/N15/N15-1113.pdf">[PDF]</a></p></li>
<li><p><strong>Collecting Highly Parallel Data for Paraphrase Evaluation</strong>
David L. Chen and William B. Dolan.
Annual Meetings of the Association for Computational Linguistics (ACL), 2011.
<a href="http://www.cs.utexas.edu/users/ml/clamp/videoDescription/">[<strong>Project Page</strong>]</a>
<a href="http://www.cs.utexas.edu/users/ml/papers/chen.acl11.pdf">[PDF]</a></p></li>
<li><p><strong>What’s Cookin’? Interpreting Cooking Videos using Text, Speech and Vision</strong>.
Jonathan Malmaud, Jonathan Huang, Vivek Rathod, Nick Johnston, Andrew Rabinovich, and Kevin Murphy.
NAACL 2015.
<a href="http://www.cs.ubc.ca/%7Emurphyk/Papers/naacl15.pdf">[PDF]</a></p></li>
<li><p><strong>Discriminative Unsupervised Alignment of Natural Language Instructions with Corresponding Video Segments</strong>.
I. Naim, Y. Song, Q. Liu, L. Huang, H. Kautz, J. Luo, and D. Gildea. 
Proc. NAACL 2015.
<a href="http://acl.cs.qc.edu/%7Elhuang/papers/naim-video.pdf">[PDF]</a></p></li>
<li><p><strong>Grounding Action Descriptions in Videos</strong>.
Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal.
TACL 2013.
<a href="http://www.aclweb.org/anthology/Q13-1003">[PDF]</a></p></li>
<li><p><strong>Translating Videos to Natural Language Using Deep Recurrent Neural Networks</strong>.
Subhashini Venugopalan, Huijun Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko.
North American Chapter of the Association for Computational Linguistics, Denver, Colorado, June 2015. (NAACL-HLT 2015)
<a href="https://www.cs.utexas.edu/%7Evsub/pdf/Translating_Videos_NAACL15.pdf">[PDF]</a>
<a href="https://github.com/vsubhashini/caffe/tree/recurrent/examples/youtube">[Code]</a></p></li>
</ul>

<h2>
<a id="3-image-captioning" class="anchor" href="#3-image-captioning" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>3. Image Captioning</strong>
</h2>

<ul>
<li><p><strong>Im2Text: Describing Images Using 1 Million Captioned Photograph.</strong>
Vicente Ordonez, Girish Kulkarni, Tamara L. Berg. s.
Neural Information Processing Systems(NIPS), 2011.
<a href="http://tlberg.cs.unc.edu/vicente/sbucaptions/">[<em>Project Page</em>]</a>
<a href="http://tamaraberg.com/papers/generation_nips2011.pdf">[PDF]</a></p></li>
<li><p>Flickr's 100 million images dataset 2012 (YFCC100M)</p></li>
</ul>

<h3>
<a id="captioned-by-crowd" class="anchor" href="#captioned-by-crowd" aria-hidden="true"><span class="octicon octicon-link"></span></a>Captioned by Crowd</h3>

<h3>
<a id="already-captioned" class="anchor" href="#already-captioned" aria-hidden="true"><span class="octicon octicon-link"></span></a>Already Captioned</h3>

<h2>
<a id="4-beyond-image-captioning" class="anchor" href="#4-beyond-image-captioning" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>4. Beyond Image Captioning</strong>
</h2>

<h2>
<a id="5-more-possibilities" class="anchor" href="#5-more-possibilities" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>5. More Possibilities</strong>
</h2>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/VisionToLanguage/Vision-To-Language-Survey">Vision to Language: A Literature Survey</a> is maintained by <a href="https://github.com/VisionToLanguage">VisionToLanguage</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>

