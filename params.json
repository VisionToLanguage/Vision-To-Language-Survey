{"name":"Vision to Language: A Literature Survey","tagline":"vision language caption NLP","body":"## **1. Introduction**\r\n\r\n## **2. Video Captioning**\r\n\r\n* **Training a Multilingual Sportscaster: Using Perceptual Context to Learn Language**.\r\nDavid L. Chen, Joohyun Kim, Raymond J. Mooney.\r\nIn Journal of Artificial Intelligence Research (JAIR) , 37, pages 397-435, 2010.\r\n[[**Project Page**]](http://www.cs.utexas.edu/~ml/clamp/sportscasting/)\r\n[[ACM]](http://dl.acm.org/citation.cfm?id=1861761)\r\n[[PDF]](https://www.jair.org/media/2962/live-2962-4903-jair.pdf)\r\n[[JAIR link]](http://www.jair.org/papers/paper2962.html)\r\n\r\n* **Grounded Language Learning from Video Described with Sentences**.\r\nH. Yu and J. M. Siskind.\r\nIn Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, 2013, *best paper award*.\r\n[[**Project Page**]](http://haonanyu.com/research/acl2013/)\r\n[[PDF]](http://haonanyu.com/research/acl2013/)\r\n\r\n* **Story-Driven Summarization for Egocentric Video**.\r\nZheng Lu and Kristen Grauman.\r\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Portland, OR, June 2013.\r\n[[**Project Page**]](http://vision.cs.utexas.edu/projects/egocentric/storydriven.html)\r\n[[PDF]](http://www.cs.utexas.edu/~grauman/papers/lu-grauman-cvpr2013.pdf)\r\n\r\n* **Movie Script Summarization as Graph-based Scene Extraction**.\r\nPhilip John Gorinski and Mirella Lapata.\r\nProc. Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL (NAACL 2015), pages 1066–1076.\r\nMay 31 – June 5, 2015.\r\n[[PDF]](http://www.aclweb.org/anthology/N/N15/N15-1113.pdf)\r\n\r\n* **Collecting Highly Parallel Data for Paraphrase Evaluation**\r\nDavid L. Chen and William B. Dolan.\r\nAnnual Meetings of the Association for Computational Linguistics (ACL), 2011.\r\n[[**Project Page**]](http://www.cs.utexas.edu/users/ml/clamp/videoDescription/)\r\n[[PDF]](http://www.cs.utexas.edu/users/ml/papers/chen.acl11.pdf)\r\n\r\n* **What’s Cookin’? Interpreting Cooking Videos using Text, Speech and Vision**.\r\nJonathan Malmaud, Jonathan Huang, Vivek Rathod, Nick Johnston, Andrew Rabinovich, and Kevin Murphy.\r\nNAACL 2015.\r\n[[PDF]](http://www.cs.ubc.ca/~murphyk/Papers/naacl15.pdf)\r\n\r\n* **Discriminative Unsupervised Alignment of Natural Language Instructions with Corresponding Video Segments**.\r\nI. Naim, Y. Song, Q. Liu, L. Huang, H. Kautz, J. Luo, and D. Gildea. \r\nProc. NAACL 2015.\r\n[[PDF]](http://acl.cs.qc.edu/~lhuang/papers/naim-video.pdf)\r\n\r\n* **Grounding Action Descriptions in Videos**.\r\nMichaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal.\r\nTACL 2013.\r\n[[PDF]](http://www.aclweb.org/anthology/Q13-1003)\r\n\r\n* **Translating Videos to Natural Language Using Deep Recurrent Neural Networks**.\r\nSubhashini Venugopalan, Huijun Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko.\r\nNorth American Chapter of the Association for Computational Linguistics, Denver, Colorado, June 2015. (NAACL-HLT 2015)\r\n[[PDF]](https://www.cs.utexas.edu/~vsub/pdf/Translating_Videos_NAACL15.pdf)\r\n[[Code]](https://github.com/vsubhashini/caffe/tree/recurrent/examples/youtube)\r\n\r\n## **3. Image Captioning**\r\n\r\n* The SBU - 1 million Flickr dataset 2011\r\n* Flickr's 100 million images dataset 2012 (YFCC100M)\r\n\r\n### Captioned by Crowd\r\n\r\n### Already Captioned\r\n\r\n## **4. Beyond Image Captioning**\r\n\r\n\r\n## **5. More Possibilities**\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}